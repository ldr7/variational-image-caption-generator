{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d7ddf1-f943-4cdd-a9e8-6cb9e5112af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "#from model import VariationalAutoEncoder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b92bcb-4d79-4bae-9bc3-41b885e24ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_DIM = 784\n",
    "H_DIM = 250\n",
    "Z_DIM = 20\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "LR_RATE = 1e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba87b5db-6ab8-41ed-9945-7e5109c9d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim=200, z_dim=20):\n",
    "        super().__init__()\n",
    "        # encoder\n",
    "        self.img_2hdim = nn.Linear(input_dim, h_dim)\n",
    "        self.hdim_2mu = nn.Linear(h_dim, z_dim)\n",
    "        self.hdim_2sigma = nn.Linear(h_dim, z_dim)\n",
    "        \n",
    "        # decoder\n",
    "        self.z_2hdim = nn.Linear(z_dim, h_dim)\n",
    "        self.hdim_2img = nn.Linear(h_dim, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    def encode(self,x):\n",
    "        h = self.img_2hdim(x)\n",
    "        h = self.relu(h)\n",
    "        mu = self.hdim_2mu(h)\n",
    "        log_sigma = self.hdim_2sigma(h)\n",
    "        return mu, log_sigma\n",
    "    def decode(self,z):\n",
    "        h = self.z_2hdim(z)\n",
    "        h = self.relu(h)\n",
    "        x = self.hdim_2img(h)\n",
    "        return torch.sigmoid(x)\n",
    "        # sigmoid done cause images are normalized between 0 and 1\n",
    "    def forward(self,x):\n",
    "        mu, log_sigma = self.encode(x)\n",
    "        epsilon = torch.randn_like(log_sigma)\n",
    "        z_reparametrized = mu + torch.exp(0.5*log_sigma)*epsilon\n",
    "        x_reconstructed = self.decode(z_reparametrized)\n",
    "        return x_reconstructed, mu, log_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992e002c-b237-4cba-b120-206f2821cc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,28*28)\n",
    "v = VariationalAutoEncoder(784)\n",
    "x_regen,mu,sigma = v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b007af-482f-41cb-930d-a68deb8fb44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_regen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ae9ffb-925a-4e15-9c02-6c73f84c269f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b032a11a-60e1-4c41-bb87-94cb128e6f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b174ba5-cce8-4b87-b9d6-762d3019b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms.ToTensor(), download=False)\n",
    "# .ToTensor() also divides the pixel values by 255\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = VariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "loss_fn = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3921ed10-1210-4202-9309-57e1513736ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469it [00:06, 69.24it/s, loss=2.01e+4]\n",
      "469it [00:06, 69.63it/s, loss=1.68e+4]\n",
      "469it [00:06, 69.92it/s, loss=1.56e+4]\n",
      "469it [00:06, 69.89it/s, loss=1.43e+4]\n",
      "469it [00:06, 68.91it/s, loss=1.39e+4]\n",
      "469it [00:07, 66.84it/s, loss=1.35e+4]\n",
      "469it [00:07, 66.91it/s, loss=1.31e+4]\n",
      "469it [00:06, 68.61it/s, loss=1.25e+4]\n",
      "469it [00:06, 69.35it/s, loss=1.25e+4]\n",
      "469it [00:06, 69.21it/s, loss=1.22e+4]\n",
      "469it [00:06, 69.91it/s, loss=1.22e+4]\n",
      "469it [00:06, 69.49it/s, loss=1.16e+4]\n",
      "469it [00:06, 69.96it/s, loss=1.09e+4]\n",
      "469it [00:06, 69.94it/s, loss=1.14e+4]\n",
      "469it [00:06, 69.61it/s, loss=1.18e+4]\n",
      "469it [00:06, 69.86it/s, loss=1.12e+4]\n",
      "469it [00:06, 70.22it/s, loss=1.11e+4]\n",
      "469it [00:06, 70.20it/s, loss=1.09e+4]\n",
      "469it [00:06, 69.71it/s, loss=1.13e+4]\n",
      "469it [00:06, 69.91it/s, loss=1.1e+4] \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(enumerate(train_loader))\n",
    "    for i,(x,y) in loop:\n",
    "        x = x.to(DEVICE)\n",
    "        x = x.view(x.shape[0], INPUT_DIM)\n",
    "        x_reconstructed, mu, log_sigma = model(x)\n",
    "        reconstruction_loss = loss_fn(x_reconstructed, x)\n",
    "        kl_div = -0.5*torch.sum(1+log_sigma - mu.pow(2) - log_sigma.exp())\n",
    "        loss = reconstruction_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7590e12a-3c3f-48e6-8b27-8ea4cc7b432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(digit, num_examples=1):\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    model.to(DEVICE)\n",
    "    \"\"\"\n",
    "    Generates (num_examples) of a particular digit.\n",
    "    Specifically we extract an example of each digit,\n",
    "    then after we have the mu, sigma representation for\n",
    "    each digit we can sample from that.\n",
    "\n",
    "    After we sample we can run the decoder part of the VAE\n",
    "    and generate examples.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    idx = 0\n",
    "    for x, y in dataset:\n",
    "        if y == idx:\n",
    "            images.append(x)\n",
    "            idx += 1\n",
    "        if idx == 10:\n",
    "            break\n",
    "\n",
    "    encodings_digit = []\n",
    "    for d in range(10):\n",
    "        with torch.no_grad():\n",
    "            mu, sigma = model.encode(images[d].view(1, 784))\n",
    "        encodings_digit.append((mu, sigma))\n",
    "\n",
    "    mu, sigma = encodings_digit[digit]\n",
    "    for example in range(num_examples):\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z = mu + sigma * epsilon\n",
    "        out = model.decode(z)\n",
    "        out = out.view(-1, 1, 28, 28)\n",
    "        save_image(out, f\"generated_{digit}_ex{example}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d47460c1-d916-4a14-af84-81ef8ca77cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    inference(idx, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0db481ae-9f49-42e5-bba7-19eb360e687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VAE...\n",
      "\tEpoch 1 complete! \tAverage Loss:  18243.09375\n",
      "\tEpoch 2 complete! \tAverage Loss:  14377.634765625\n",
      "\tEpoch 3 complete! \tAverage Loss:  13416.01953125\n",
      "\tEpoch 4 complete! \tAverage Loss:  12115.57421875\n",
      "\tEpoch 5 complete! \tAverage Loss:  11224.568359375\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training VAE...\")\n",
    "model.train()\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "model.to(DEVICE)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(DEVICE)\n",
    "        x = x.view(x.shape[0], INPUT_DIM)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_fn(x_hat,x)\n",
    "        \n",
    "        overall_loss = loss.item()\n",
    "        kl_div = -0.5*torch.sum(1+log_var - mean.pow(2) - log_var.exp())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss)\n",
    "    \n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41881237-1d14-41e2-841a-707fd642903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "/ (batch_idx*BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ath_pytorch",
   "language": "python",
   "name": "ath_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
