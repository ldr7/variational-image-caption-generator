{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe4494dc-3e92-44b8-bd39-42f896630621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f8c2fb3-20ae-414d-a167-98a7cb9050eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(nn.Linear(img_features,128),\n",
    "                                 nn.LeakyReLU(0.01),\n",
    "                                 nn.Linear(128,1),\n",
    "                                 nn.Sigmoid())\n",
    "    def forward(self,x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0b75967-48f6-4203-9c72-5177b18273cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_features, z_features):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(nn.Linear(z_features, 256),\n",
    "                                nn.LeakyReLU(0.1),\n",
    "                                nn.Linear(256,img_features),\n",
    "                                nn.Tanh())  # inputs and outputs are in the range [-1,1]\n",
    "    def forward(self,x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c5d8b86-f319-49b4-8367-a8b6c27c2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(image_dim,z_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fd1aef4-d7bb-4a20-8363-c1534357ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8822314-c3c2-4494-bf9e-65ed3b553fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e68282e-a95d-47c3-9486-0e7c5382b263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32ff99df-df93-4f20-9819-d4b8c3246261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7cb76c18-e74c-40bf-99d9-b8f1e314b471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/1875                       Loss D: 0.6591, loss G: 0.7523\n",
      "Epoch [1/50] Batch 0/1875                       Loss D: 0.6397, loss G: 0.8196\n",
      "Epoch [2/50] Batch 0/1875                       Loss D: 0.7097, loss G: 0.7650\n",
      "Epoch [3/50] Batch 0/1875                       Loss D: 0.6835, loss G: 0.7706\n",
      "Epoch [4/50] Batch 0/1875                       Loss D: 0.6479, loss G: 0.8503\n",
      "Epoch [5/50] Batch 0/1875                       Loss D: 0.6843, loss G: 0.9461\n",
      "Epoch [6/50] Batch 0/1875                       Loss D: 0.6475, loss G: 0.8681\n",
      "Epoch [7/50] Batch 0/1875                       Loss D: 0.7365, loss G: 0.9128\n",
      "Epoch [8/50] Batch 0/1875                       Loss D: 0.6958, loss G: 0.8614\n",
      "Epoch [9/50] Batch 0/1875                       Loss D: 0.6042, loss G: 0.9767\n",
      "Epoch [10/50] Batch 0/1875                       Loss D: 0.7599, loss G: 0.7792\n",
      "Epoch [11/50] Batch 0/1875                       Loss D: 0.7263, loss G: 0.8289\n",
      "Epoch [12/50] Batch 0/1875                       Loss D: 0.5790, loss G: 0.8731\n",
      "Epoch [13/50] Batch 0/1875                       Loss D: 0.6087, loss G: 1.0190\n",
      "Epoch [14/50] Batch 0/1875                       Loss D: 0.6706, loss G: 0.8657\n",
      "Epoch [15/50] Batch 0/1875                       Loss D: 0.6459, loss G: 0.8856\n",
      "Epoch [16/50] Batch 0/1875                       Loss D: 0.6375, loss G: 1.0253\n",
      "Epoch [17/50] Batch 0/1875                       Loss D: 0.6677, loss G: 0.8550\n",
      "Epoch [18/50] Batch 0/1875                       Loss D: 0.6006, loss G: 0.9351\n",
      "Epoch [19/50] Batch 0/1875                       Loss D: 0.5919, loss G: 1.0307\n",
      "Epoch [20/50] Batch 0/1875                       Loss D: 0.6539, loss G: 0.9759\n",
      "Epoch [21/50] Batch 0/1875                       Loss D: 0.6586, loss G: 0.9384\n",
      "Epoch [22/50] Batch 0/1875                       Loss D: 0.7231, loss G: 0.8850\n",
      "Epoch [23/50] Batch 0/1875                       Loss D: 0.6167, loss G: 0.7623\n",
      "Epoch [24/50] Batch 0/1875                       Loss D: 0.5841, loss G: 0.8948\n",
      "Epoch [25/50] Batch 0/1875                       Loss D: 0.5962, loss G: 0.7463\n",
      "Epoch [26/50] Batch 0/1875                       Loss D: 0.5422, loss G: 1.0536\n",
      "Epoch [27/50] Batch 0/1875                       Loss D: 0.6189, loss G: 0.8945\n",
      "Epoch [28/50] Batch 0/1875                       Loss D: 0.7226, loss G: 0.7902\n",
      "Epoch [29/50] Batch 0/1875                       Loss D: 0.5759, loss G: 0.9265\n",
      "Epoch [30/50] Batch 0/1875                       Loss D: 0.6591, loss G: 0.9084\n",
      "Epoch [31/50] Batch 0/1875                       Loss D: 0.6680, loss G: 0.9087\n",
      "Epoch [32/50] Batch 0/1875                       Loss D: 0.6941, loss G: 0.8255\n",
      "Epoch [33/50] Batch 0/1875                       Loss D: 0.6812, loss G: 0.9721\n",
      "Epoch [34/50] Batch 0/1875                       Loss D: 0.8185, loss G: 0.8236\n",
      "Epoch [35/50] Batch 0/1875                       Loss D: 0.6571, loss G: 1.0909\n",
      "Epoch [36/50] Batch 0/1875                       Loss D: 0.6523, loss G: 0.8327\n",
      "Epoch [37/50] Batch 0/1875                       Loss D: 0.5368, loss G: 1.1602\n",
      "Epoch [38/50] Batch 0/1875                       Loss D: 0.6438, loss G: 0.9548\n",
      "Epoch [39/50] Batch 0/1875                       Loss D: 0.6212, loss G: 0.8295\n",
      "Epoch [40/50] Batch 0/1875                       Loss D: 0.5828, loss G: 1.1549\n",
      "Epoch [41/50] Batch 0/1875                       Loss D: 0.6511, loss G: 0.9372\n",
      "Epoch [42/50] Batch 0/1875                       Loss D: 0.5940, loss G: 1.1063\n",
      "Epoch [43/50] Batch 0/1875                       Loss D: 0.5433, loss G: 0.9496\n",
      "Epoch [44/50] Batch 0/1875                       Loss D: 0.6915, loss G: 0.8015\n",
      "Epoch [45/50] Batch 0/1875                       Loss D: 0.5701, loss G: 1.0571\n",
      "Epoch [46/50] Batch 0/1875                       Loss D: 0.5797, loss G: 0.9974\n",
      "Epoch [47/50] Batch 0/1875                       Loss D: 0.5360, loss G: 1.0583\n",
      "Epoch [48/50] Batch 0/1875                       Loss D: 0.6551, loss G: 0.8651\n",
      "Epoch [49/50] Batch 0/1875                       Loss D: 0.5613, loss G: 1.1353\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real,y) in enumerate(loader): # note that the label is not used - unsupervised!!\n",
    "        real = real.view(-1,784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        # Train Discriminator max(log(D(x)) + log(1-D(G(z))))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        # will be using BCEloss terms to get this. in fake pass ones to use only ylog(x) term and zeros for real to use 1-y(log(1-x)) term\n",
    "        disc_real = disc(real).view(-1)\n",
    "        disc_loss_real = criterion(disc_real, torch.ones_like(disc_real,device=device))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        disc_loss_fake = criterion(disc_fake, torch.zeros_like(disc_fake,device=device))\n",
    "        disc_loss = 0.5*(disc_loss_real + disc_loss_fake)\n",
    "        opt_disc.zero_grad()\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Train Generator min(log(1-D(G(z)))) <-> max(log(D(G(z)))) due to saturating gradients in the first option\n",
    "        output = disc(fake).view(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output, device=device))\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {disc_loss:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "#                 writer_fake.add_image(\n",
    "#                     \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "#                 )\n",
    "#                 writer_real.add_image(\n",
    "#                     \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "#                 )\n",
    "                save_image(fake[0], f\"generated/generated_ex{y[0]}.png\")\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c008dbf-8ba5-4536-a7a0-26f516ffcb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ath_pytorch",
   "language": "python",
   "name": "ath_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
